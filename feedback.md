# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** ppawar03-byte
**Raw Score:** 46/50 (92.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (9/10 = 90.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Correct use of t-SNE on MNIST and clear 2D visualization colored by labels. Meets the goal. Consider tuning perplexity or subsampling for speed/clarity on large datasets, but your solution is functionally correct.

**Part ex1-part2** (ex1-part2.code): 2/3 points

_Feedback:_ Good attempt: you trained KNN on t-SNE features and reported accuracy with a brief comparison. However, you refit t-SNE on the test set, making train/test embeddings incomparable—invalidating the KNN evaluation. Fit t-SNE once (e.g., on full X) then split before KNN.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct: You trained KNN on UMAP-transformed training data, transformed the test set with the same UMAP model, predicted, and printed accuracy. This meets the objective and uses UMAP appropriately. Nice job.

---

### Exercise 4 (17/20 = 85.0%)

**Part ex2-part1** (ex2-part1.code): 5/7 points

_Feedback:_ Good attempt: applies PCA, trains KNN, reports accuracy, and visualizes 2D/3D. However, you fit PCA on the full dataset before the split (data leakage). Fit PCA on X_train and transform both train/test. Using n_components instead of PCA(.9) is acceptable.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Well done: you applied UMAP (2D/3D), evaluated with KNN, and visualized results, consistent with your prior PCA workflow. Minor suggestion: fit UMAP on the training set and transform the test set to avoid data leakage, but not penalized here.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ You compared PCA vs UMAP clearly, noted parameter effects, and explored 2D/3D—good. However, to earn full credit, you should mention that UMAP often works better in low dimensions with low n_neighbors. Your claim that PCA is better isn’t tied to that key setting.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Correct PCA to 2D and proper scatter colored by class. Code should run given X_mnist_train and y_mnist_train. Plot is appropriate; imports included. Nice job. Optionally labeling axes or noting explained variance would be a bonus but not required.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Correct: you refit PCA on full feature space and plotted the first 40 components’ explained variance ratio. The scree plot meets the requirement, and labeling is acceptable. Nice job.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach. You use pca_full from prior work, compute cumulative explained variance, and find the smallest number of components to reach 95% with argmax+1. This meets the task requirements and should work as intended.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Good job. You used the n_components from Step 4, reduced and reconstructed the data, and visualized the same digit via the provided plotting function. This meets the task requirements and correctly ties to your prior work.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Well done. You compare KNN with and without PCA and correctly preserve ~80% variance using PCA(n_components=0.80). You fit PCA on training data and transform test data properly, then evaluate both models. Meets the step’s objectives.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*